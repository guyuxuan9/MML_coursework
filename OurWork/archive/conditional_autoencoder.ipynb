{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (33600, 24) (33600,)\n",
      "Validation set: (7200, 24) (7200,)\n",
      "Test set: (7200, 24) (7200,)\n",
      "Scaled features using StandardScaler.\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timothychung/opt/anaconda3/envs/ml2/lib/python3.10/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['features_input', 'labels_input']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Arguments `target` and `output` must have the same shape. Received: target.shape=(None, 2), output.shape=(None, 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 125\u001b[0m\n\u001b[1;32m    119\u001b[0m early_stop \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# 5. Train the Conditional Autoencoder with explicit output matching\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_onehot\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Input: Features and labels for concatenation\u001b[39;49;00m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdecoded_output\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclassifier_output\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_onehot\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Targets for each output\u001b[39;49;00m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_val_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_onehot\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdecoded_output\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclassifier_output\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_onehot\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m    134\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# 6. Extract 2D Embeddings Using the Encoder\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# Define encoder to include label input\u001b[39;00m\n\u001b[1;32m    141\u001b[0m encoder \u001b[38;5;241m=\u001b[39m Model(inputs\u001b[38;5;241m=\u001b[39m[input_features, input_labels], outputs\u001b[38;5;241m=\u001b[39mencoded_output)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml2/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml2/lib/python3.10/site-packages/keras/src/backend/tensorflow/nn.py:587\u001b[0m, in \u001b[0;36mcategorical_crossentropy\u001b[0;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e1, e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape, output\u001b[38;5;241m.\u001b[39mshape):\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e1 \u001b[38;5;241m!=\u001b[39m e2:\n\u001b[0;32m--> 587\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    588\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArguments `target` and `output` must have the same shape. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    590\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    591\u001b[0m         )\n\u001b[1;32m    593\u001b[0m output, from_logits \u001b[38;5;241m=\u001b[39m _get_logits(\n\u001b[1;32m    594\u001b[0m     output, from_logits, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    595\u001b[0m )\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_logits:\n",
      "\u001b[0;31mValueError\u001b[0m: Arguments `target` and `output` must have the same shape. Received: target.shape=(None, 2), output.shape=(None, 24)"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 1. Import Necessary Libraries\n",
    "# -------------------------------\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# Suppress TensorFlow and Keras warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Data Preparation\n",
    "# -------------------------------\n",
    "\n",
    "# Load data\n",
    "X = np.load('Datasets/kryptonite-24-X.npy')\n",
    "y = np.load('Datasets/kryptonite-24-y.npy')\n",
    "X = np.round(X)\n",
    "\n",
    "# Split into training (70%), validation (15%), and test sets (15%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"Training set:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation set:\", X_val.shape, y_val.shape)\n",
    "print(\"Test set:\", X_test.shape, y_test.shape)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Feature Scaling\n",
    "# -------------------------------\n",
    "\n",
    "# Standardize features to have zero mean and unit variance\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Scaled features using StandardScaler.\")\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Conditional Autoencoder Setup\n",
    "# -------------------------------\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train_onehot = tf.keras.utils.to_categorical(y_train, num_classes=2)\n",
    "y_val_onehot = tf.keras.utils.to_categorical(y_val, num_classes=2)\n",
    "y_test_onehot = tf.keras.utils.to_categorical(y_test, num_classes=2)\n",
    "\n",
    "# Define input dimensions\n",
    "input_dim = X_train_scaled.shape[1]  # Should be 24\n",
    "encoding_dim = 2  # For 2D visualization\n",
    "\n",
    "# Inputs for features and labels\n",
    "input_features = Input(shape=(input_dim,), name='features_input')\n",
    "input_labels = Input(shape=(2,), name='labels_input')  # Assuming binary classification\n",
    "\n",
    "# Concatenate features and labels for the encoder\n",
    "merged_input = concatenate([input_features, input_labels])\n",
    "\n",
    "# Encoder network\n",
    "encoded = Dense(64, activation='relu')(merged_input)\n",
    "encoded = BatchNormalization()(encoded)\n",
    "encoded = Dropout(0.2)(encoded)\n",
    "\n",
    "encoded = Dense(32, activation='relu')(encoded)\n",
    "encoded = BatchNormalization()(encoded)\n",
    "encoded = Dropout(0.2)(encoded)\n",
    "\n",
    "encoded_output = Dense(encoding_dim, activation='relu', name='encoded_output')(encoded)\n",
    "\n",
    "# Concatenate encoded representation with labels for the decoder\n",
    "decoder_input = concatenate([encoded_output, input_labels])\n",
    "\n",
    "# Decoder network\n",
    "decoded = Dense(32, activation='relu')(decoder_input)\n",
    "decoded = BatchNormalization()(decoded)\n",
    "decoded = Dropout(0.2)(decoded)\n",
    "\n",
    "decoded = Dense(64, activation='relu')(decoded)\n",
    "decoded = BatchNormalization()(decoded)\n",
    "\n",
    "# Define the final decoded output with 24 units to match the shape of X\n",
    "decoded_output = Dense(input_dim, activation='sigmoid', name='decoded_output')(decoded)\n",
    "\n",
    "# Classification Head\n",
    "classifier = Dense(16, activation='relu')(encoded_output)\n",
    "classifier = BatchNormalization()(classifier)\n",
    "classifier = Dropout(0.2)(classifier)\n",
    "classifier_output = Dense(2, activation='softmax', name='classifier_output')(classifier)\n",
    "\n",
    "# Define the combined model\n",
    "model = Model(inputs=[input_features, input_labels], outputs=[decoded_output, classifier_output])\n",
    "\n",
    "# Compile the model with correct loss functions and target-output matching\n",
    "model.compile(optimizer='adam',\n",
    "              loss={'decoded_output': 'mean_squared_error',  # Reconstruction loss for continuous data\n",
    "                    'classifier_output': 'categorical_crossentropy'},  # Classification loss\n",
    "              loss_weights={'decoded_output': 1.0, 'classifier_output': 1.0},\n",
    "              metrics={'classifier_output': 'accuracy'})\n",
    "\n",
    "# Define Early Stopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Train the Conditional Autoencoder with explicit output matching\n",
    "# -------------------------------\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train_scaled, y_train_onehot],  # Input: Features and labels for concatenation\n",
    "    {'decoded_output': X_train_scaled, 'classifier_output': y_train_onehot},  # Targets for each output\n",
    "    epochs=100,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    validation_data=([X_val_scaled, y_val_onehot], {'decoded_output': X_val_scaled, 'classifier_output': y_val_onehot}),\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Extract 2D Embeddings Using the Encoder\n",
    "# -------------------------------\n",
    "\n",
    "# Define encoder to include label input\n",
    "encoder = Model(inputs=[input_features, input_labels], outputs=encoded_output)\n",
    "\n",
    "# Get the embeddings\n",
    "X_train_encoded = encoder.predict([X_train_scaled, y_train_onehot])\n",
    "X_val_encoded = encoder.predict([X_val_scaled, y_val_onehot])\n",
    "X_test_encoded = encoder.predict([X_test_scaled, y_test_onehot])\n",
    "\n",
    "print(\"Encoded Training Set:\", X_train_encoded.shape)\n",
    "print(\"Encoded Validation Set:\", X_val_encoded.shape)\n",
    "print(\"Encoded Test Set:\", X_test_encoded.shape)\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Visualization Function\n",
    "# -------------------------------\n",
    "\n",
    "def plot_embedding(X, y, title):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    palette = sns.color_palette(\"hsv\", len(np.unique(y)))\n",
    "    sns.scatterplot(x=X[:,0], y=X[:,1], hue=y, palette=palette, alpha=0.6)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Component 1')\n",
    "    plt.ylabel('Component 2')\n",
    "    plt.legend(title='Class')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Plot Autoencoder Embeddings\n",
    "print(\"Conditional Autoencoder Visualization:\")\n",
    "plot_embedding(X_train_encoded, y_train, \"Conditional Autoencoder - Training Set\")\n",
    "plot_embedding(X_val_encoded, y_val, \"Conditional Autoencoder - Validation Set\")\n",
    "plot_embedding(X_test_encoded, y_test, \"Conditional Autoencoder - Test Set\")\n",
    "\n",
    "# -------------------------------\n",
    "# 8. Training a Classifier on the Encoded Embeddings\n",
    "# -------------------------------\n",
    "\n",
    "# Define Early Stopping for the classifier\n",
    "classifier_early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Define the classifier model\n",
    "classifier = Sequential([\n",
    "    Input(shape=(encoding_dim,)),  # encoding_dim = 2\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(2, activation='softmax')  # Output layer with 2 neurons for categorical classification\n",
    "])\n",
    "\n",
    "# Compile the classifier\n",
    "classifier.compile(optimizer=Adam(),\n",
    "                   loss='categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "# Train the classifier\n",
    "history_classifier = classifier.fit(\n",
    "    X_train_encoded, y_train_onehot,\n",
    "    epochs=100,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_val_encoded, y_val_onehot),\n",
    "    callbacks=[classifier_early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 9. Evaluating the Classifier\n",
    "# -------------------------------\n",
    "\n",
    "# Predict probabilities on the test set\n",
    "y_pred_prob = classifier.predict(X_test_encoded)\n",
    "# Convert probabilities to class labels (0 or 1)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Convert true labels from one-hot to integers\n",
    "y_true = np.argmax(y_test_onehot, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "test_accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Generate classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Class 0', 'Class 1'],\n",
    "            yticklabels=['Class 0', 'Class 1'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (33600, 24) (33600,)\n",
      "Validation set: (7200, 24) (7200,)\n",
      "Test set: (7200, 24) (7200,)\n",
      "Scaled features using StandardScaler.\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timothychung/opt/anaconda3/envs/ml2/lib/python3.10/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['features_input', 'labels_input']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Arguments `target` and `output` must have the same shape. Received: target.shape=(None, 2), output.shape=(None, 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 122\u001b[0m\n\u001b[1;32m    116\u001b[0m early_stop \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# 5. Train the Conditional Autoencoder\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_onehot\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Inputs: scaled features and one-hot labels\u001b[39;49;00m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdecoded_output\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclassifier_output\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_onehot\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Targets: features and labels\u001b[39;49;00m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_val_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_onehot\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdecoded_output\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclassifier_output\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_onehot\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m    131\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# 6. Extract 2D Embeddings Using the Encoder\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# Define encoder to include label input\u001b[39;00m\n\u001b[1;32m    138\u001b[0m encoder \u001b[38;5;241m=\u001b[39m Model(inputs\u001b[38;5;241m=\u001b[39m[input_features, input_labels], outputs\u001b[38;5;241m=\u001b[39mencoded_output)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml2/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml2/lib/python3.10/site-packages/keras/src/backend/tensorflow/nn.py:587\u001b[0m, in \u001b[0;36mcategorical_crossentropy\u001b[0;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e1, e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape, output\u001b[38;5;241m.\u001b[39mshape):\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e1 \u001b[38;5;241m!=\u001b[39m e2:\n\u001b[0;32m--> 587\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    588\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArguments `target` and `output` must have the same shape. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    590\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    591\u001b[0m         )\n\u001b[1;32m    593\u001b[0m output, from_logits \u001b[38;5;241m=\u001b[39m _get_logits(\n\u001b[1;32m    594\u001b[0m     output, from_logits, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    595\u001b[0m )\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_logits:\n",
      "\u001b[0;31mValueError\u001b[0m: Arguments `target` and `output` must have the same shape. Received: target.shape=(None, 2), output.shape=(None, 24)"
     ]
    }
   ],
   "source": [
    "# # -------------------------------\n",
    "# # 1. Import Necessary Libraries\n",
    "# # -------------------------------\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Model, Sequential\n",
    "# from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, concatenate\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# # -------------------------------\n",
    "# # 2. Data Preparation\n",
    "# # -------------------------------\n",
    "\n",
    "# # Load data\n",
    "# X = np.load('Datasets/kryptonite-24-X.npy')\n",
    "# y = np.load('Datasets/kryptonite-24-y.npy')\n",
    "# X = np.round(X)\n",
    "\n",
    "# # Split into training (70%), validation (15%), and test sets (15%)\n",
    "# X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "#     X, y, test_size=0.3, random_state=42, stratify=y\n",
    "# )\n",
    "# X_val, X_test, y_val, y_test = train_test_split(\n",
    "#     X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    "# )\n",
    "\n",
    "# print(\"Training set:\", X_train.shape, y_train.shape)\n",
    "# print(\"Validation set:\", X_val.shape, y_val.shape)\n",
    "# print(\"Test set:\", X_test.shape, y_test.shape)\n",
    "\n",
    "# # -------------------------------\n",
    "# # 3. Feature Scaling\n",
    "# # -------------------------------\n",
    "\n",
    "# # Standardize features to have zero mean and unit variance\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_train)\n",
    "\n",
    "# X_train_scaled = scaler.transform(X_train)\n",
    "# X_val_scaled = scaler.transform(X_val)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# print(\"Scaled features using StandardScaler.\")\n",
    "\n",
    "# # -------------------------------\n",
    "# # 4. Conditional Autoencoder Setup\n",
    "# # -------------------------------\n",
    "\n",
    "# # One-hot encode labels\n",
    "# y_train_onehot = tf.keras.utils.to_categorical(y_train, num_classes=2)\n",
    "# y_val_onehot = tf.keras.utils.to_categorical(y_val, num_classes=2)\n",
    "# y_test_onehot = tf.keras.utils.to_categorical(y_test, num_classes=2)\n",
    "\n",
    "# # Define input dimensions\n",
    "# input_dim = X_train_scaled.shape[1]\n",
    "# encoding_dim = 2  # For 2D visualization\n",
    "\n",
    "# # Modify input to include label information\n",
    "# input_features = Input(shape=(input_dim,), name='features_input')\n",
    "# input_labels = Input(shape=(2,), name='labels_input')  \n",
    "\n",
    "# # Concatenate features and labels\n",
    "# merged_input = concatenate([input_features, input_labels])\n",
    "\n",
    "# # Encoder\n",
    "# encoded = Dense(64, activation='relu')(merged_input)\n",
    "# encoded = BatchNormalization()(encoded)\n",
    "# encoded = Dropout(0.2)(encoded)\n",
    "\n",
    "# encoded = Dense(32, activation='relu')(encoded)\n",
    "# encoded = BatchNormalization()(encoded)\n",
    "# encoded = Dropout(0.2)(encoded)\n",
    "\n",
    "# encoded_output = Dense(encoding_dim, activation='relu', name='encoded_output')(encoded)\n",
    "\n",
    "# # Decoder: concatenate encoded representation with labels\n",
    "# decoder_input = concatenate([encoded_output, input_labels])\n",
    "\n",
    "# decoded = Dense(32, activation='relu')(decoder_input)\n",
    "# decoded = BatchNormalization()(decoded)\n",
    "# decoded = Dropout(0.2)(decoded)\n",
    "\n",
    "# decoded = Dense(64, activation='relu')(decoded)\n",
    "# decoded = BatchNormalization()(decoded)\n",
    "# decoded_output = Dense(input_dim, activation='sigmoid', name='decoded_output')(decoded)\n",
    "\n",
    "# # Classification Head\n",
    "# classifier = Dense(16, activation='relu')(encoded_output)\n",
    "# classifier = BatchNormalization()(classifier)\n",
    "# classifier = Dropout(0.2)(classifier)\n",
    "# classifier_output = Dense(2, activation='softmax', name='classifier_output')(classifier)\n",
    "\n",
    "# # Define the combined model\n",
    "# model = Model(inputs=[input_features, input_labels], outputs=[decoded_output, classifier_output])\n",
    "\n",
    "# # Compile the model\n",
    "# # model.compile(optimizer='adam',\n",
    "# #               loss={'decoded_output': 'binary_crossentropy',\n",
    "# #                     'classifier_output': 'categorical_crossentropy'},\n",
    "# #               loss_weights={'decoded_output': 1.0, 'classifier_output': 1.0},\n",
    "# #               metrics={'classifier_output': 'accuracy'})\n",
    "\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss={'decoded_output': 'mean_squared_error',  # Suitable for continuous data in X\n",
    "#                     'classifier_output': 'categorical_crossentropy'},\n",
    "#               loss_weights={'decoded_output': 1.0, 'classifier_output': 1.0},\n",
    "#               metrics={'classifier_output': 'accuracy'})\n",
    "\n",
    "# # Define Early Stopping\n",
    "# early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# # -------------------------------\n",
    "# # 5. Train the Conditional Autoencoder\n",
    "# # -------------------------------\n",
    "\n",
    "# history = model.fit(\n",
    "#     [X_train_scaled, y_train_onehot],  # Inputs: scaled features and one-hot labels\n",
    "#     {'decoded_output': X_train_scaled, 'classifier_output': y_train_onehot},  # Targets: features and labels\n",
    "#     epochs=100,\n",
    "#     batch_size=256,\n",
    "#     shuffle=True,\n",
    "#     validation_data=([X_val_scaled, y_val_onehot], {'decoded_output': X_val_scaled, 'classifier_output': y_val_onehot}),\n",
    "#     callbacks=[early_stop],\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# # -------------------------------\n",
    "# # 6. Extract 2D Embeddings Using the Encoder\n",
    "# # -------------------------------\n",
    "\n",
    "# # Define encoder to include label input\n",
    "# encoder = Model(inputs=[input_features, input_labels], outputs=encoded_output)\n",
    "\n",
    "# # Get the embeddings\n",
    "# X_train_encoded = encoder.predict([X_train_scaled, y_train_onehot])\n",
    "# X_val_encoded = encoder.predict([X_val_scaled, y_val_onehot])\n",
    "# X_test_encoded = encoder.predict([X_test_scaled, y_test_onehot])\n",
    "\n",
    "# print(\"Encoded Training Set:\", X_train_encoded.shape)\n",
    "# print(\"Encoded Validation Set:\", X_val_encoded.shape)\n",
    "# print(\"Encoded Test Set:\", X_test_encoded.shape)\n",
    "\n",
    "# # -------------------------------\n",
    "# # 7. Visualization Function\n",
    "# # -------------------------------\n",
    "\n",
    "# def plot_embedding(X, y, title):\n",
    "#     plt.figure(figsize=(8,6))\n",
    "#     palette = sns.color_palette(\"hsv\", len(np.unique(y)))\n",
    "#     sns.scatterplot(x=X[:,0], y=X[:,1], hue=y, palette=palette, alpha=0.6)\n",
    "#     plt.title(title)\n",
    "#     plt.xlabel('Component 1')\n",
    "#     plt.ylabel('Component 2')\n",
    "#     plt.legend(title='Class')\n",
    "#     plt.grid(True)\n",
    "#     plt.show()\n",
    "\n",
    "# # Plot Autoencoder Embeddings\n",
    "# print(\"Conditional Autoencoder Visualization:\")\n",
    "# plot_embedding(X_train_encoded, y_train, \"Conditional Autoencoder - Training Set\")\n",
    "# plot_embedding(X_val_encoded, y_val, \"Conditional Autoencoder - Validation Set\")\n",
    "# plot_embedding(X_test_encoded, y_test, \"Conditional Autoencoder - Test Set\")\n",
    "\n",
    "# # -------------------------------\n",
    "# # 8. Training a Classifier on the Encoded Embeddings\n",
    "# # -------------------------------\n",
    "\n",
    "# # Define Early Stopping for the classifier\n",
    "# classifier_early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# # Define the classifier model\n",
    "# classifier = Sequential([\n",
    "#     Input(shape=(encoding_dim,)),  # encoding_dim = 2\n",
    "#     Dense(16, activation='relu'),\n",
    "#     Dense(8, activation='relu'),\n",
    "#     Dense(2, activation='softmax')  # Output layer with 2 neurons for categorical classification\n",
    "# ])\n",
    "\n",
    "# # Compile the classifier\n",
    "# classifier.compile(optimizer=Adam(),\n",
    "#                    loss='categorical_crossentropy',\n",
    "#                    metrics=['accuracy'])\n",
    "\n",
    "# # Train the classifier\n",
    "# history_classifier = classifier.fit(\n",
    "#     X_train_encoded, y_train_onehot,\n",
    "#     epochs=100,\n",
    "#     batch_size=256,\n",
    "#     shuffle=True,\n",
    "#     validation_data=(X_val_encoded, y_val_onehot),\n",
    "#     callbacks=[classifier_early_stop],\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# # -------------------------------\n",
    "# # 9. Evaluating the Classifier\n",
    "# # -------------------------------\n",
    "\n",
    "# # Predict probabilities on the test set\n",
    "# y_pred_prob = classifier.predict(X_test_encoded)\n",
    "# # Convert probabilities to class labels (0 or 1)\n",
    "# y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# # Convert true labels from one-hot to integers\n",
    "# y_true = np.argmax(y_test_onehot, axis=1)\n",
    "\n",
    "# # Calculate accuracy\n",
    "# test_accuracy = accuracy_score(y_true, y_pred)\n",
    "# print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# # Generate classification report\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(y_true, y_pred))\n",
    "\n",
    "# # Generate confusion matrix\n",
    "# conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# # Plot confusion matrix\n",
    "# plt.figure(figsize=(6,5))\n",
    "# sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "#             xticklabels=['Class 0', 'Class 1'],\n",
    "#             yticklabels=['Class 0', 'Class 1'])\n",
    "# plt.xlabel('Predicted Label')\n",
    "# plt.ylabel('True Label')\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.show()\n",
    "\n",
    "# # -------------------------------\n",
    "# # 10. (Optional) Visualize Classifier Decision Boundary\n",
    "# # -------------------------------\n",
    "\n",
    "# # def plot_decision_boundary(model, X, y, title):\n",
    "# #     # Define the grid range\n",
    "# #     x_min, x_max = X[:,0].min() - 1, X[:,0].max() + 1\n",
    "# #     y_min, y_max = X[:,1].min() - 1, X[:,1].max() + 1\n",
    "# #     h = 0.01  # step size in the mesh\n",
    "\n",
    "# #     # Generate a grid of points\n",
    "# #     xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "# #                          np.arange(y_min, y_max, h))\n",
    "# #     grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "# #     # Predict probabilities for each point in the grid\n",
    "# #     probs = model.predict(grid)\n",
    "# #     Z = np.argmax(probs, axis=1).reshape(xx.shape)\n",
    "\n",
    "# #     # Plot contour and training examples\n",
    "# #     plt.figure(figsize=(10,8))\n",
    "# #     plt.contourf(xx, yy, Z, levels=[-0.1, 0.5, 1.1], colors=['#FFAAAA', '#AAAAFF'], alpha=0.5)\n",
    "# #     palette = sns.color_palette(\"hsv\", len(np.unique(y)))\n",
    "# #     sns.scatterplot(x=X[:,0], y=X[:,1], hue=y, palette=palette, alpha=0.6)\n",
    "# #     plt.title(title)\n",
    "# #     plt.xlabel('Component 1')\n",
    "# #     plt.ylabel('Component 2')\n",
    "# #     plt.legend(title='Class')\n",
    "# #     plt.show()\n",
    "\n",
    "# # # Plot decision boundary on the test set\n",
    "# # plot_decision_boundary(classifier, X_test_encoded, y_test, \"Classifier Decision Boundary on Test Set\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
